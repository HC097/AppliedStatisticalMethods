---
title: "lab4"
author: "Harley Combest"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_float: yes
    theme: spacelab
    highlight: pygments
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Task 1

```{r}
getwd()
```

## Task 2

```{r}
spruce.df=read.csv("SPRUCE.csv")
tail(spruce.df)
```

## Task 3

```{r}
#load s20x library and make lowess smoother
library(s20x)

trendscatter(Height~BHDiameter,f=0.5,data=spruce.df)
# Now make the linear model
spruce.lm=lm(Height~BHDiameter,data=spruce.df)
#residuals  created from the linear model object
height.res=residuals(spruce.lm)
#fitted values made from the linear model object
height.fit=fitted(spruce.lm)
#Make the plot using the plot function 
plot(height.res,height.fit)
# Put a lowess smoother through res vs fitted
trendscatter( height.res,height.fit)
# Quick way to make a residual plot
plot(spruce.lm, which =1)
# Checking normality
normcheck(spruce.lm,shapiro.wilk = TRUE)
```

The shape of the curve in residuals vs fitted values using trendscatter() is am imperfect normal distribution in shape. The other curve mentioned looks like something akin to an exponential distribution in shape.

The p-value for the Shapiro-Wilk Test in this case is .29. If the p-value is greater than .05, then the null hypothesis is accepted. This is the case here;i.e., we accept the NULL hypothesis is normally distributed data.

The issue with applying the straight line to this data set is that it doesn't truly capture the trend of the distribution as well as this current model.

## Task 4

```{r}
quad.lm=lm(Height~BHDiameter + I(BHDiameter^2),data=spruce.df)
#scatter plot of Height vs BHDiameter
plot(Height~BHDiameter,bg="Blue",pch=21,cex=1.2,
ylim=c(0,max(Height)),xlim=c(0,max(BHDiameter)), 
main="Spruce height prediction",data=spruce.df)
#adding quadtratic curve
abline(quad.lm)
#making a vector of fitted values
quad.fit=fitted(quad.lm)
#making a plot of residuals vs fitted values
plot(quad.lm, which=1)
#making a QQ plot
normcheck(quad.lm,shapiro.wilk = TRUE)
```

P-value is .684. I accept the Null Hypothesis that this data is normally distributed 

## Task 5

```{r}
#Summarizing quad.lm
summary(quad.lm)
#Making interval estimates for mentioned variables
quad.lm$coef[1] + c(-2.205022,2.205022)
quad.lm$coef[2] + c(-0.243786,0.243786)
quad.lm$coef[3] + c(-0.006635,0.006635)
#predicting Height of spruce for mentioned values
predict(quad.lm, data.frame(BHDiameter=c(15,18,20)))
#comparing two models with anova
anova(quad.lm)
anova(spruce.lm)
#Finding TSS,MSS,RSS,and MSS/TSS
RSS=with(spruce.df, sum((Height-quad.fit)^2))
RSS
MSS = with(spruce.df, sum((quad.fit-mean(Height))^2))
MSS
TSS = with(spruce.df, sum((Height-mean(Height))^2))
TSS
MSS/TSS
```

The asked for values are as follows: 0.860896, 1.469592, and -0.027457.

Equation of fitted line: 0.86089580 +1.46959217*x  -0.02745726*x^2 where x is BHDiameter.

These predictions are taller on average than the previous predictions.

Multiple R-Squared is .7741. It's value is also higher than in the previous model. An indication of a better fit.

In this context, the multiple R-squared tells us that our model is providing a great prediction of height based on BHDiameter.

This model explains the most variability in Height.

Residual values for the nonquadratic are larger on average, more degrees of freedom with the quadratic, and F value is much larger in the quadratic than the non quadratic.

## Task 6

```{r}
cooks20x(quad.lm)
#Now remove the 24th datum and reanalyze data
quad2.lm=lm(Height~BHDiameter + I(BHDiameter^2) , data=spruce.df[-24,])
summary(quad2.lm)
summary(quad.lm)
```

Cook's distance illustrates the impact of each observation on the fitted response values when performing a least squares regression analysis by measuring the effect of deleting a given obseration. Points with large Cook's distance carry a heavier influence on the data.

For this model, it tells us that observations 18, 21, and 24 carry the largest potential to distort the outcome and accuracy of the regression.

Multiple R-squared is larger for the model with removed datum; this indicated a greater goodness of fit for this model in comparison to the unaltered version.

## Task 7

Proof.
$$
l1:y=\beta_0+\beta_1x\\
l2:y=\beta_0+\delta+(\beta_1+\zeta)x\\
$$

We have the two line intersecting at the change point such that

$$
\beta_0+\beta_1 x_k =\beta_0+\sigma+(\beta_1+\zeta)x_k\\
\implies\\
\sigma=-\zeta x_k\\
\implies\\
l2: y=\beta_0-\zeta x_k + (\beta_1+\zeta)x=\beta_0+\beta_1 x+\zeta(x-x_k).
$$

Introducing an indicator function that is 1 when x>x_k and 0 otherwise yields

$$
y=\beta_0+\beta_1x+\zeta(x-x_k)I(x>x_k)=\beta_0+\beta_1x+\beta_2(x-x_k)I(x>x_k).
$$

```{r}
sp2.df=within(spruce.df, X<-(BHDiameter-20)*(BHDiameter>20)) # this makes a new variable and places it within the same df
sp2.df

lmp=lm(Height~BHDiameter + X,data=sp2.df)
tmp=summary(lmp)
names(tmp)
myf = function(x,coef){
  coef[1]+coef[2]*(x) + coef[3]*(x-18)*(x-18>0)
}
plot(spruce.df,main="Piecewise regression")
myf(0, coef=tmp$coefficients[,"Estimate"])
curve(myf(x,coef=tmp$coefficients[,"Estimate"] ),add=TRUE, lwd=2,col="Blue")
abline(v=18)
text(18,16,paste("R sq.=",round(tmp$r.squared,4) ))
```

## Task 8

```{r}
library(myPackage1)
myFunction(5)
```

myFunction(n) makes 10 barplots generated from random samples of n objects.



